{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook shows the overhead in using the inferSchema option when reading in text files in SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First some intialisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import datetime\n",
    "sc = pyspark.SparkContext(appName=\"read-big-file\")\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time how long it takes in to read an approx 6.5 million record text file using inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "# put the the path to your file in the .load statement\n",
    "df = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"false\").option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", '|') \\\n",
    "    .load(\"file:///d:/tmp/iholding/myfiles/issue50.txt\")\n",
    "\n",
    "print(\"Nr of records = \", df.count())\n",
    "print(datetime.datetime.now())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and the same without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "# put the the path to your file in the .load statement\n",
    "df = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"delimiter\", '|') \\\n",
    "    .load(\"file:///d:/tmp/iholding/myfiles/issue50.txt\")\n",
    "\n",
    "print(\"Nr of records = \", df.count())\n",
    "print(datetime.datetime.now())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we really wanta  schema - and we usually do - then we can pre-create one\n",
    "## that fits our data and apply it to the input file\n",
    "##\n",
    "## There are two ways of doing this .... do it in code like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, \\\n",
    "StructType, DoubleType, LongType,IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType([\n",
    "        StructField(\"column1\", IntegerType(), True),\n",
    "        StructField(\"column2\", IntegerType(), True),\n",
    "        StructField(\"column3\", IntegerType(), True),\n",
    "        StructField(\"column4\", TimestampType(), True),\n",
    "        StructField(\"column5\", IntegerType(), True),\n",
    "        StructField(\"column6\", LongType(), True),\n",
    "        StructField(\"column7\", IntegerType(), True),\n",
    "        StructField(\"column8\", DoubleType(), True),\n",
    "        StructField(\"column9\", DoubleType(), True),\n",
    "        StructField(\"column10\",LongType(), True),\n",
    "        StructField(\"column11\", DoubleType(), True),\n",
    "        StructField(\"column12\", IntegerType(), True),\n",
    "        StructField(\"column13\", IntegerType(), True)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now see what timing difference there is compared to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "# put the the path to your file in the .load statement\n",
    "df = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"false\").schema(myschema)\\\n",
    "    .option(\"delimiter\", '|') \\\n",
    "    .load(\"file:///d:/tmp/iholding/myfiles/issue50.txt\")\n",
    "\n",
    "print(\"Nr of records = \", df.count())\n",
    "print(datetime.datetime.now())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .... or construct a little dummy text file that matches our real input file, use the inferSchema on it\n",
    "### and apply it to our real input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyDf = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"false\").option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\", '|').load(\"file:///d:/tmp/downloads/tinyfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "# put the the path to your file in the .load statement\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"false\").schema(dummyDf.schema)\\\n",
    "    .option(\"delimiter\", '|') \\\n",
    "    .load(\"file:///d:/tmp/iholding/myfiles/issue50.txt\")\n",
    "print(\"Nr of records = \", df.count())\n",
    "print(datetime.datetime.now())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
